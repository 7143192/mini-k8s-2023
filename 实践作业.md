# `k8s`实践作业

520021910279 李昱翰

## 使用`kubeadm`安装并部署`k8s`集群

**`Q1`**:请记录所有安装步骤的指令，并简要描述其含义

- 首先配置`dns`解析：

  ```
  #master:
  #通过如下指令来配置master节点的域名
  $ sudo hostnamectl set-hostname k8s-master
  
  #node:
  #通过如何指令来配置worker节点的域名
  $ sudo hostnamectl set-hostname k8s-node
  ```

- 之后安装`containerd`:

  ```
  sudo apt install \
  ca-certificates \
  curl \
  gnupg \
  lsb-release
  
  sudo mkdir -p /etc/apt/keyrings
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
    $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  
  sudo apt update
  sudo apt install containerd.io
  ```

- 修改相关配置：

  ```
  #创建containerd配置文件目录
  mkdir -p /etc/containerd
  #导出配置文件方便之后修改
  containerd config default | sudo tee /etc/containerd/config.toml
  #修改配置文件，主要是修改pause容器所使用的镜像，将其替换为国内源。
  vim /etc/containerd/config.toml
  #创建镜像加速的目录 
  mkdir /etc/containerd/certs.d/docker.io -pv
  #配置加速
  cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF
  server = "https://docker.io"
  [host."https://xxxxxx.mirror.aliyuncs.com"]
    capabilities = ["pull", "resolve"]
  EOF
  ```

- 加载依赖组件：

  ```
  #加载containerd的内核模块
  cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
  overlay
  br_netfilter
  EOF
  sudo modprobe overlay
  sudo modprobe br_netfilter
  #重启containerd
  systemctl restart containerd 
  systemctl status containerd
  #安装依赖组件
  sudo apt-get install -y apt-transport-https ca-certificates curl
  #配置相关密钥
  sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg
  
  sudo echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] http://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
  ```

- 安装`k8s`:

  ```
  sudo apt-get update
  #安装kubelet，kubectl，kubeadm
  sudo apt-get install -y kubelet kubeadm kubectl
  #启动kubelet
  systemctl enable --now kubelet
  ```

- 安装`k8s`成功之后相关配置：

  ```
  #运行配置文件
  kubeadm init --config config.yaml
  #配置网络插件，此处安装的是calico插件。
  wget https://docs.projectcalico.org/v3.15/manifests/calico.yaml
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  export KUBECONFIG=/etc/kubernetes/admin.conf
  #最后在node节点运行：
  kubeadm join
  ```



**`Q2`**:在两个节点上分别使用 `ps aux | grep kube` 列出所有和`k8s`相关的进程，记录其输出，并简
要说明各个进程的作用

**`master`节点**：

![1.png](./1.png)

各个进程作用：

- `kubelet`进程：运行在`master node`上，用于管理`master node`上所有的`pod`，如进行资源收集，`pod`状态检查等，并用于处理`master`分发到当前`node`的作用。
- `api-server`进程：`api-server`只会运行在`master`节点上，主要作用是提供系统所需的`REST`接口，来处理系统所有的对于`API`对象的读取以及修改，并且起到了不同组件之间的通信枢纽的作用。
- `kube-scheduler`进程:只会运行在`master`节点上，主要作用是当系统增加新的`pod`的时候，根据一定的调度策略来选择这个`pod`归属于的`node`。
- `etcd`进程：`k8s`使用键值数据库`etcd`进行数据存储，所以这个进程就是在运行数据库进程。
- `kube-controller-manager`进程：运行在`master`上，`k8s`中需要对于每一种`workload`运行一个`controller`来进行这一类对象的监控以及管理，而`kube-controller-manager`进程就是在运行这些`controllers`。
- `kube-proxy`进程：`kube-proxy`负责为Service提供cluster内部的服务发现和负载均衡.该进程即是在`master`后台运行`kube-proxy`进程。

**`worker`节点**:

![2.png](./2.png)

各个进程作用：

在`worker`节点上面只运行`kubelet`以及`kubeproxy`进程，作用与`master`节点相同。



**`Q3`**:在两个节点中分别使用 `crictl ps` 显示所有正常运行的`containerd`容器，记录其输出，并简要
说明各个容器所包含的`k8s`组件，以及那些`k8s`组件未运行在容器中

**`master`节点**：

![3.png](./3.png)

各个容器所包含的`k8s`组件：

从上到下(按照图中顺序)依次包含了：

**`coredns`,`calico`,`kube-proxy`,`kube-controller-manager`,`kube-apiserver`,`kube-shceduler`,`etcd`。**

没有运行在容器中的组件：

**`kubelet`**。

**`worker`节点**：

![4.png](./4.png)

包含的`k8s`组件：

**`coredns`,`calico`, `kube-proxy`**。

没有运行在容器中的`k8s`组件：

**`kubelet`**。`worker`节点上面不会运行`shceduler,apiserver,kube-controller`等组件。



## 部署`pod`

**`Q4`**:请采用声明式接口对Pod进行部署，并将部署所需的`yaml`文件记录在实践文档中

使用的`yaml`文件：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: container1
      image: hejingkai/fileserver:latest
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - name: shared
          mountPath: /usr/share/files
      ports:
        - containerPort: 8080
    - name: container2
      image: hejingkai/downloader:latest
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - name: shared
          mountPath: /data
      ports:
        - containerPort: 3000
  volumes:
    - name: shared
      emptyDir: {}
```

部署使用的声明式指令：`kubectl apply -f pod1.yaml(配置文件文件名)`



**`Q5`**:请在`worker`节点上，在部署`Pod`的前后分别采用`crictl ps`查看所有运行中的容器并对比两者
的区别。请将创建该Pod所创建的全部新容器列举出来，并一一解释其作用.

**部署`pod`之前**：

![4.png](./4.png)

**部署`pod`之后**：

![5.png](./5.png)

**新创建的容器**：

`3dce7be356170-container2(downloader)`,运行`downloader`镜像。

`9f149df0d3125-container1(fileserver)`,运行`fileserver`镜像。

(**注**：理论上还应该有一个`pause container`，但是没有显示，可能是被`k8s`隐藏了，因为`pause container`确实不是用户配置的，而是`k8s`额外自动创建的。若想查看可以使用如下指令查看：)

`ctr -n k8s.io c ls`,结果如下：

![14.png](./14.png)



**`Q6`**:请结合博客https://blog.51cto.com/u_15069443/4043930的内容，将容器中的`veth`与`host`机器
上的`veth`匹配起来，并采用`ip link`和`ip addr`指令找到位于host机器中的所有网络设备及其之
间的关系。结合两者的输出，试绘制出`worker`节点中涉及新部署`Pod`的所有网络设备和其网络结构，
并在图中标注出从`master`节点中使用`podip`访问位于`worker`节点中的`Pod`的网络路径。

(**注**：若直接使用容器无法运行`ip`系列指令，可以如下操作：

`crictl inspect CONTAINER_ID | grep pid`，

之后运行`sudo nsenter -u -n -p -t PID`,之后就可以正确执行了。)

**`container1(fileserver)`输出结果：**

```yaml
# ip link
root@pod1:~# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
# ip addr
root@pod1:~# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.16.113.142/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b843:f0ff:fe86:27e2/64 scope link
       valid_lft forever preferred_lft forever
```

**`container2(downloader)`输出结果**：

```yaml
# ip link
root@pod1:~# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
# ip addr
root@pod1:~# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.16.113.142/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b843:f0ff:fe86:27e2/64 scope link
       valid_lft forever preferred_lft forever
```

**`worker node`输出结果：**

```yaml
# ip link
root@k8s-node:~# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether fa:16:3e:97:f5:9d brd ff:ff:ff:ff:ff:ff
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:c0:2d:e1:88 brd ff:ff:ff:ff:ff:ff
4: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
12: cali710b62eb583@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-512c7ed4-c8a6-006a-dad6-b979ab060f2c
25: calice0906292e2@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-67811830-2497-e49e-2486-f048d3c97c21

# ip addr
root@k8s-node:~# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP group default qlen 1000
    link/ether fa:16:3e:97:f5:9d brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.7/24 brd 192.168.1.255 scope global dynamic ens3
       valid_lft 65644sec preferred_lft 65644sec
    inet6 fe80::f816:3eff:fe97:f59d/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:c0:2d:e1:88 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
4: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 172.16.113.128/32 brd 172.16.113.128 scope global tunl0
       valid_lft forever preferred_lft forever
12: cali710b62eb583@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-512c7ed4-c8a6-006a-dad6-b979ab060f2c
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
       valid_lft forever preferred_lft forever
25: calice0906292e2@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-67811830-2497-e49e-2486-f048d3c97c21
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
       valid_lft forever preferred_lft forever
```

**`master node`输出：**

```yaml
# ip link
root@k8s-master:~# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether fa:16:3e:18:2e:cf brd ff:ff:ff:ff:ff:ff
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:01:7d:eb:1d brd ff:ff:ff:ff:ff:ff
4: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
11: cali47a1d697c2e@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-02c76ffb-596c-0ecf-2d15-a52876348690
12: cali8fb9235bdf3@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-aef3545b-7e46-d5c0-f813-a31f7bdb47cf
```

从上述输出可以知道，与`eth0`有关输出如下：

```yaml
# container1
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0

# container2
4: eth0@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP mode DEFAULT group default
    link/ether ba:43:f0:86:27:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0

# worker host node
25: calice0906292e2@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-67811830-2497-e49e-2486-f048d3c97c21
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
       valid_lft forever preferred_lft forever
```

所以**此次创建的`pod`的`eth0`网卡是与`worker host`节点上面的`calice0906292e2`虚拟网卡配对的**。

**`worker`节点中涉及新部署`Pod`的所有网络设备和其网络结构**:

![6.png](./6.png)

**从`master`节点中使用`podip`访问位于`worker`节点中的`Pod`的网络路径**:

`master`中的`PodIP`通过`iptables`路由找到`worker node`上对应的`calico`网卡，之后通过`calico`网卡来访问`pod`的`eth0`网卡，从而实现通过`podIP`访问`pod`的功能。

路径如下图：

![7.png](./7.png)



## 使用`deployment`为`pod`创建服务

**`Q7`:**请采用声明式接口对`Deployment`进行部署，并将Deployment所需要的`yaml`文件记录在文档中

使用的`yaml`配置文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment1
spec:
  replicas: 3
  selctor:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: fileserver
          image: hejingkai/fileserver:latest
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: shared
              mountPath: /usr/share/files
        - name: downloader
          image: hejingkai/downloader:latest
          ports:
            - containerPort: 3000
          volumeMounts:
             - name: shared
               mountPath: /data
      volumes:
        - name: shared
          emptyDir: {}
```

使用的声明式指令：`kubectl apply -f deployment.yaml(deployment配置文件名)`

之后使用`kubectl get deployments`指令查看可以知道成功创建了`deployment`:

![8.png](./8.png)



**`Q8`:**在该使用`Deployment`的部署方式下，不同`Pod`之间的文件是否共享？该情况会在实际使用文件下
载与共享服务时产生怎样的实际效果与问题？应如何解决这一问题？

- 不同`pod`之间的文件不会被共享。

- 在实际使用的时候，会出现每个`pod`各自下载自己的文件而不会共享的情况。

- 可能引入的问题就是，如果需要在3个`replica pods`之间进行共享的话，那么就无法正确共享而出现备份之间的不一致性问题。

- 解决方式：

  使用外部存储`volume`来存储需要进行共享的文件。



## 部署`service`

**`Q9`:**请采用声明式接口对`Service`进行部署，并将部署所需的`yaml`文件记录在实践文档中

`fileserver`对应服务使用的`yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: fileserver-service
spec:
  selector:
    app: my-app #应该与之前deployment指定的APP label取值相同
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP
```

`downloader`对应的服务使用的`yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: downloader-service
spec:
  selector:
    app: my-app
  ports:
    - name: http
      port: 3000
      targetPort: 3000
  type: ClusterIP
```

使用的声明式指令：

`kubectl apply -f service.yaml, kubectl apply -f service1.yaml`

使用`kubectl get services`指令进行验证：

![9.png](./9.png)

使用`describe`相关指令进行验证：

![10.png](./10.png)

![11.png](./11.png)



**`Q10`:**请在`master`节点中使用 `iptables-save` 指令输出所有的`iptables`规则，将其中与`Service`访问
相关的`iptable`规则记录在实践文档中，并解释网络流量是如何采用基于`iptables`的方式被从对`Service`
的`clusterIP`的访问定向到实际的`Pod`中的，又是如何实现负载均衡到三个`pod`的。

**与服务相关的`iptables`规则：**

```yaml
:PREROUTING ACCEPT [3089067:442639378]
:OUTPUT ACCEPT [3100232:456930954]
:PREROUTING ACCEPT [32248:1935548]
:INPUT ACCEPT [3088909:442591251]
:OUTPUT ACCEPT [3100078:456883311]
:POSTROUTING ACCEPT [3100082:456883795]
:INPUT ACCEPT [3772:538165]
:OUTPUT ACCEPT [3781:552013]
:OUTPUT ACCEPT [51:3068]
:POSTROUTING ACCEPT [51:3068]
:KUBE-SEP-63WEJZSGGINXNIQC - [0:0]
:KUBE-SEP-LRZCWGRLNPS2S5FI - [0:0]
:KUBE-SEP-N337LIXF2EAV756R - [0:0]
:KUBE-SEP-N6QGS2ILZZ2U7SKT - [0:0]
:KUBE-SEP-P5SBNJLMOCE2FGDH - [0:0]
:KUBE-SEP-STBQJFPD5L7WJPRA - [0:0]
:KUBE-SVC-7AHFP3Z4YL7KTY7Q - [0:0]
:KUBE-SVC-JO6MBZ6ZH66GBF5A - [0:0]
#KUBE-SEP链路
-A KUBE-SEP-63WEJZSGGINXNIQC -s 172.16.113.150/32 -m comment --comment "default/fileserver-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-63WEJZSGGINXNIQC -p tcp -m comment --comment "default/fileserver-service:http" -m tcp -j DNAT --to-destination 172.16.113.150:8080
-A KUBE-SEP-LRZCWGRLNPS2S5FI -s 172.16.113.149/32 -m comment --comment "default/fileserver-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-LRZCWGRLNPS2S5FI -p tcp -m comment --comment "default/fileserver-service:http" -m tcp -j DNAT --to-destination 172.16.113.149:8080
-A KUBE-SEP-N337LIXF2EAV756R -s 172.16.113.150/32 -m comment --comment "default/downloader-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-N337LIXF2EAV756R -p tcp -m comment --comment "default/downloader-service:http" -m tcp -j DNAT --to-destination 172.16.113.150:3000
-A KUBE-SEP-N6QGS2ILZZ2U7SKT -s 172.16.113.151/32 -m comment --comment "default/downloader-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-N6QGS2ILZZ2U7SKT -p tcp -m comment --comment "default/downloader-service:http" -m tcp -j DNAT --to-destination 172.16.113.151:3000
-A KUBE-SEP-P5SBNJLMOCE2FGDH -s 172.16.113.149/32 -m comment --comment "default/downloader-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-P5SBNJLMOCE2FGDH -p tcp -m comment --comment "default/downloader-service:http" -m tcp -j DNAT --to-destination 172.16.113.149:3000
-A KUBE-SEP-STBQJFPD5L7WJPRA -s 172.16.113.151/32 -m comment --comment "default/fileserver-service:http" -j KUBE-MARK-MASQ
-A KUBE-SEP-STBQJFPD5L7WJPRA -p tcp -m comment --comment "default/fileserver-service:http" -m tcp -j DNAT --to-destination 172.16.113.151:8080
# KUBE-SERVICES 链路
-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
-A KUBE-SERVICES -d 10.105.123.104/32 -p tcp -m comment --comment "default/fileserver-service:http cluster IP" -m tcp --dport 8080 -j KUBE-SVC-7AHFP3Z4YL7KTY7Q
-A KUBE-SERVICES -d 10.102.151.149/32 -p tcp -m comment --comment "default/downloader-service:http cluster IP" -m tcp --dport 3000 -j KUBE-SVC-JO6MBZ6ZH66GBF5A
-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
# KUBE-SVC-链路
-A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.149:8080" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-LRZCWGRLNPS2S5FI
-A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.150:8080" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-63WEJZSGGINXNIQC
-A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.151:8080" -j KUBE-SEP-STBQJFPD5L7WJPRA
-A KUBE-SVC-JO6MBZ6ZH66GBF5A -m comment --comment "default/downloader-service:http -> 172.16.113.149:3000" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-P5SBNJLMOCE2FGDH
-A KUBE-SVC-JO6MBZ6ZH66GBF5A -m comment --comment "default/downloader-service:http -> 172.16.113.150:3000" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-N337LIXF2EAV756R
-A KUBE-SVC-JO6MBZ6ZH66GBF5A -m comment --comment "default/downloader-service:http -> 172.16.113.151:3000" -j KUBE-SEP-N6QGS2ILZZ2U7SKT
```

**网络流量如何定位到实际的`pod`:**

- 主要涉及到`KUBE-SERVICE`,`KUBE_SVC-`,`KUBE-SEP-`，`KUBE-MARK-MASQ`几条新添加的`iptables`链路。

- 在外界要访问一个`service`的时候，会优先访问`KUBE-SERVICES`链路，并将相应的请求以及需要的端口信息以**`TCP`**的形式转发给`KUBE-SVC-`链路：

  ```yaml
  -A KUBE-SERVICES -d 10.105.123.104/32 -p tcp -m comment --comment "default/fileserver-service:http cluster IP" -m tcp --dport 8080 -j KUBE-SVC-7AHFP3Z4YL7KTY7Q
  -A KUBE-SERVICES -d 10.102.151.149/32 -p tcp -m comment --comment "default/downloader-service:http cluster IP" -m tcp --dport 3000 -j KUBE-SVC-JO6MBZ6ZH66GBF5A
  ```

- 之后`KUBE-SVC-`链路会根据指定的概率进行随机的负载均衡操作，如在当前场景下，访问3个`fileserver`备份的实际概率为分别为：0.33，0.33，0.33,`downloader`同理。

  ```yaml
  -A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.149:8080" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-LRZCWGRLNPS2S5FI #0.33
  -A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.150:8080" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-63WEJZSGGINXNIQC #0.33,因为要在第一条没有选中才会考虑第二条
  -A KUBE-SVC-7AHFP3Z4YL7KTY7Q -m comment --comment "default/fileserver-service:http -> 172.16.113.151:8080" -j KUBE-SEP-STBQJFPD5L7WJPRA #0.33，因为只剩下0.33的概率了
  ```

- 之后`KUBE-SVC-`链路会将响应请求转发到`KUBE-SEP`链路。

  ```yaml
  -A KUBE-SEP-63WEJZSGGINXNIQC -s 172.16.113.150/32 -m comment --comment "default/fileserver-service:http" -j KUBE-MARK-MASQ
  -A KUBE-SEP-P5SBNJLMOCE2FGDH -p tcp -m comment --comment "default/downloader-service:http" -m tcp -j DNAT --to-destination 172.16.113.149:3000
  ```

- 最后`KUBE-SEP-`链路将相应请求转发到`KUBE-MARK-`链路(即`DNAT`)来进行最终的请求处理操作。

**如何进行`pod`之间的负载均衡**：

- 结合上述内容可以知道，**通过`KUBE-SVC-`链路指定的不同规则的概率进行随机的负载均衡操作**，在此问题中，最终效果为3个备份的最终被分配到的流量相同。

  

**`Q11`:**`kube-proxy`组件在整个`service`的定义与实现过程中起到了什么作用？请自行查找资料，并解释
在`iptables`模式下，`kube-proxy`的功能

**`kube-proxy`组件在`service`定义和实现过程中起到的作用：**

`kube-proxy`负责为`Service`提供`cluster`内部的服务发现和负载均衡，它运行在每个`Node`节点上，负责`Pod`网络代理, 它会定时从`etcd`服务获取到`service`信息来做相应的策略，维护网络规则和负载均衡工作。

同时,`kube-proxy`也是对于`service`的访问入口，包括包括集群内`Pod`到`Service`的访问和集群外访问`service`。

**`iptables`模式下，`kube-proxy`的功能**：

`kube-proxy` 组件负责维护` node` 节点上的防火墙规则和路由规则，在` iptables` 模式下，会根据 `Service` 以及  endpoints 对象的改变实时刷新规则，`kube-proxy` 使用了 `iptables` 的 filter 表和 `nat` 表，并对  `iptables` 的链进行了扩充，自定义了 `KUBE-SERVICES`等多条新的链路，并通过这些新的链路以及已有链路来拦截数据包并进行相应的请求跳转处理操作。



## 部署`HPA`

**`Q12`:**请在上面部署的`Deployment`的基础上为其配置`HPA`，并将部署所需的`yaml`文件记录在实践文档
中，如果对上面的`Deployment`配置有修改也请在文档中说明。具体参数为最大副本数为6，最小副本
数为3，目标`cpu`平均利用率为40%。

新增的`hpa.yaml`文件：

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: hpa1
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: deployment1
  minReplicas: 3
  maxReplicas: 6
  targetCPUUtilizationPercentage: 40
```

运行需要的声明式指令：

`kubectl apply -f newDeploy.yaml -f hpa.yaml`

使用`kubectl get/describe horizontalpodautoscaler`验证结果如下：

![12.png](./12.png)



**`Q13`:**小明发现，`hpa`缩容过快在负载抖动的情况下会引起`pod`被不必要地删除和再次创建，因此他决定
限制缩容的速率，请为`hpa`配置缩容的速率限制为每分钟10%，并将部署所需的`yaml`文件记录在实践
文档中。

修改之前的`hpa.yaml`文件内容如下：

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa2
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: deployment1
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 40
  behavior:
    scaleDown:
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
```

使用的声明式指令：

`kubectl apply -f hpaDeploy.yaml -f hpa1.yaml`

使用指令`kubectl get/describe horizontalpodautoscaler`验证结果如下：

![13.png](./13.png)
